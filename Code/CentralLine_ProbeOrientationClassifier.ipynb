{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Line: Probe Orientation Classifier\n",
    "\n",
    "General pipeline:\n",
    "1. Take in an image and preprocess it\n",
    "2. Use unet to generate segmented image\n",
    "3. Use transfer learning to define pretrained network\n",
    "4. convolve image, softmax activation to get 3 output classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "## Define user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths\n",
    "input_dir = r\"C:\\repos\\aigt\\DeepLearnLive\\Datasets\\US_Vessel_Segmentations\"\n",
    "# unet_path = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Data\\Logs\\Output_CentralLineUNET_1\\Central_Line_UNet_model_2020-11-28_10-33-36\"\n",
    "output_dir = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Paper\"\n",
    "# output_dir = r\"C:\\Users\\cbarr\\Desktop\\classifier_output\"\n",
    "\n",
    "unet_fold_0 = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\IMNO Outputs\\Central_Line_UNet_fold_0_2020-12-04_18-38-12\"\n",
    "unet_fold_1 = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\IMNO Outputs\\Central_Line_UNet_fold_1_2020-12-04_18-56-38\"\n",
    "unet_fold_2 = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\IMNO Outputs\\Central_Line_UNet_fold_2_2020-12-04_19-06-04\"\n",
    "unet_fold_3 = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\IMNO Outputs\\Central_Line_UNet_fold_3_2020-12-04_19-14-59\"\n",
    "\n",
    "#Learning parameters\n",
    "image_size = 128\n",
    "\n",
    "test_idx = [15,16,17,18,19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Input, GlobalAveragePooling2D, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS01-20200210-132740\n",
      "MS01-20200210-132740 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22801c033fdd4f37b445049961e7e634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1749)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS01-20200210-133541\n",
      "MS01-20200210-133541 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e80b8c7996a47f08e82aebd186c679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1123)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS01-20200210-134522\n",
      "MS01-20200210-134522 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b49ca952dd493782d49069e99c229b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1088)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS01-20200210-135109\n",
      "MS01-20200210-135109 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983b50ff7d9b4a22bc6db1115afd6de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1005)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS01-20200210-135709\n",
      "MS01-20200210-135709 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cb777b58914273b7b72091ed2a3fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=830)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS02-20200210-152131\n",
      "MS02-20200210-152131 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc550c12b1e4f27aedf6b12bcf2096b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1319)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS02-20200210-152827\n",
      "MS02-20200210-152827 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3d754bdbf64dd5a37d1d7addcd5dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1059)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS02-20200210-153709\n",
      "MS02-20200210-153709 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb6b6c289b74551896968ce19c3c2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=979)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS02-20200210-154411\n",
      "MS02-20200210-154411 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7517970089054d8986ac28e275010bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=833)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS02-20200210-160404\n",
      "MS02-20200210-160404 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0241d95fd9134bd2bef20493b0750710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1389)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS03-20200213-152826\n",
      "MS03-20200213-152826 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309179aaa3c44284bc9a8942cc8be3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1218)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS03-20200213-153647\n",
      "MS03-20200213-153647 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bdf659faf14f94a21d0aeecc95118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1214)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS03-20200213-154347\n",
      "MS03-20200213-154347 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7626ec2ce7d74a51b50a14554582425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1076)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS03-20200213-155250\n",
      "MS03-20200213-155250 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c0b9caffd3417684720a02f37758d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1256)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS03-20200213-155823\n",
      "MS03-20200213-155823 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57868956954646359a122e004ad8264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=663)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS04-20200213-173406\n",
      "MS04-20200213-173406 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9983ad17bc49490484bd1d9e7287a453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1663)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS04-20200213-174259\n",
      "MS04-20200213-174259 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1fa59d65f94c5097e3baab9ce7c51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1188)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS04-20200213-175619\n",
      "MS04-20200213-175619 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e704d15a714c07ac5cd0dcfba87260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1070)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS04-20200213-180237\n",
      "MS04-20200213-180237 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed38c87e4ba4f2eb84e5976f436e492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1120)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current sequence name: MS04-20200213-180907\n",
      "MS04-20200213-180907 Ultrasound processing progress: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9f43cb9e554a3ebf7b3d7f4143d336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1070)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-37d9b4a76369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m#Aggregate all the collected data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mall_us_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mus_data_by_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_by_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "sequence_names = [name for name in os.listdir(input_dir)]\n",
    "\n",
    "def process_us(us_img, output_size):\n",
    "    resized_img = cv2.resize(us_img, (output_size, output_size)).astype(np.float16)\n",
    "    return resized_img / resized_img.max()\n",
    "\n",
    "def accumulate_and_process(all_img_paths, processing_fxn, output_size, f): \n",
    "    data = np.array([], dtype=np.float64).reshape(output_size, output_size, 0)\n",
    "    for idx, img_path in enumerate(all_img_paths):\n",
    "        f.value = idx\n",
    "        img = cv2.imread(os.path.abspath(img_path), 0)\n",
    "        processed_img = processing_fxn(img, output_size)\n",
    "        data = np.dstack((data, processed_img))\n",
    "    return data\n",
    "\n",
    "us_data_by_seq = []\n",
    "labels_by_seq = []\n",
    "\n",
    "for seq in sequence_names:\n",
    "    \n",
    "    print(\"current sequence name: \" + seq)\n",
    "    \n",
    "    us_paths = list(Path(input_dir + \"\\\\\" + seq).glob(seq+\"_[0-9][0-9][0-9][0-9][0-9].png\"))\n",
    "    \n",
    "    f_us = IntProgress(min=0, max=len(us_paths))                    \n",
    "    print(seq + \" Ultrasound processing progress: \")\n",
    "    display(f_us)\n",
    "                         \n",
    "    #Accumulate and process images\n",
    "    us_data = accumulate_and_process(us_paths, process_us, image_size, f_us)\n",
    "    \n",
    "    #Read in the labels for this data\n",
    "    labels_path = os.path.join((input_dir + \"\\\\\" + seq), (seq+\"_Labels.csv\"))\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "    \n",
    "    #Save the numpy files\n",
    "    output_us_path = os.path.join(output_dir, seq +\"_ultrasound\" )\n",
    "    output_seg_path = os.path.join(output_dir, seq+\"_label\" )\n",
    "    np.save(output_us_path, us_data)\n",
    "    np.save(output_seg_path, labels_df)\n",
    "    \n",
    "    #Add additional axis and rearrange\n",
    "#     us_data = us_data[...,np.newaxis]\n",
    "#     us_data = us_data.transpose(2,1,0,3)\n",
    "    \n",
    "    #Verify that the length of labels is equal to the length of us_data\n",
    "#     print(\"Number of us images: {} | Number of labels: {}\".format(us_data.shape[0], labels_df.shape[0]))\n",
    "    \n",
    "    #Append the us data to the list\n",
    "#     us_data_by_seq.append(us_data)\n",
    "#     labels_by_seq.append(labels_df)\n",
    "\n",
    "# #Aggregate all the collected data\n",
    "# all_us_data = np.concatenate(us_data_by_seq, axis=0)\n",
    "# all_labels = pd.concat(labels_by_seq)\n",
    "\n",
    "# #Extract only the labels of the orientations\n",
    "# orientation_labels = np.array(all_labels.Probe_Orientation)\n",
    "\n",
    "# #Specify the output file name\n",
    "# output_path = os.path.join(output_dir, \"all_data.npz\")\n",
    "\n",
    "# #Save the data to the file\n",
    "# np.savez(output_path, us_data = all_us_data, labels = orientation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and seperate into training / validation / testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Problem: Dataset 0 has 1749 ultrasounds and 1749 labels\n",
      "Data Problem: Dataset 1 has 1123 ultrasounds and 1123 labels\n",
      "Data Problem: Dataset 2 has 1088 ultrasounds and 1088 labels\n",
      "Data Problem: Dataset 3 has 1005 ultrasounds and 1005 labels\n",
      "Data Problem: Dataset 4 has 830 ultrasounds and 830 labels\n",
      "Data Problem: Dataset 5 has 1319 ultrasounds and 1319 labels\n",
      "Data Problem: Dataset 6 has 1059 ultrasounds and 1059 labels\n",
      "Data Problem: Dataset 7 has 979 ultrasounds and 979 labels\n",
      "Data Problem: Dataset 8 has 833 ultrasounds and 833 labels\n",
      "Data Problem: Dataset 9 has 1389 ultrasounds and 1389 labels\n",
      "Data Problem: Dataset 10 has 1218 ultrasounds and 1218 labels\n",
      "Data Problem: Dataset 11 has 1214 ultrasounds and 1214 labels\n",
      "Data Problem: Dataset 12 has 1076 ultrasounds and 1076 labels\n",
      "Data Problem: Dataset 13 has 1256 ultrasounds and 1256 labels\n",
      "Data Problem: Dataset 14 has 663 ultrasounds and 663 labels\n",
      "Data Problem: Dataset 15 has 1663 ultrasounds and 1663 labels\n",
      "Data Problem: Dataset 16 has 1188 ultrasounds and 1188 labels\n",
      "Data Problem: Dataset 17 has 1070 ultrasounds and 1070 labels\n",
      "Data Problem: Dataset 18 has 1120 ultrasounds and 1120 labels\n",
      "Data Problem: Dataset 19 has 1070 ultrasounds and 1070 labels\n"
     ]
    }
   ],
   "source": [
    "#Read the US image and segmentations paths into lists\n",
    "data_input_dir = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Data\\ClassifierDataNp\"\n",
    "label_files = list(sorted(Path(data_input_dir).glob(\"*_label.npy\")))\n",
    "us_files = list(sorted(Path(data_input_dir).glob(\"*_ultrasound.npy\")))\n",
    "\n",
    "#Go through all file paths in both arrays and append to list\n",
    "us_data_by_seq = []\n",
    "label_by_seq = []\n",
    "for i in range(len(us_files)):\n",
    "    \n",
    "    #Load the current files as 3D numpy arrays\n",
    "    us_np = np.load(os.path.abspath(us_files[i]))\n",
    "    label_np = np.load(os.path.abspath(label_files[i]), allow_pickle=True)\n",
    "    \n",
    "    #Normalize and add channel dimension\n",
    "    us_np = us_np[...,np.newaxis] / 255\n",
    "    \n",
    "    #Append to the collector lists.\n",
    "    us_data_by_seq.append(us_np.transpose(2,1,0,3))\n",
    "    label_by_seq.append(label_np[:,4])\n",
    "    \n",
    "    \n",
    "#Verify that the total number of segmentation images matches the total number of us images.\n",
    "for idx in range(len(us_data_by_seq)):\n",
    "    if len(label_by_seq[idx][0]) != len(us_data_by_seq[idx][0]):\n",
    "        print(\"Data Problem: Dataset {} has {} ultrasounds and {} labels\". format(\n",
    "            idx, us_data_by_seq[idx].shape[0], len(label_by_seq[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\repos\\dlenv\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#Create lists of indices for training and validation sets\n",
    "trainAndVal_idx = list(range(len(us_data_by_seq)))\n",
    "trainAndVal_idx = [x for x in trainAndVal_idx if x not in test_idx] #Remove IDs\n",
    "\n",
    "#Extract and concatenate the labels for train / test / val\n",
    "y_trainAndVal = np.concatenate(np.take(label_by_seq, trainAndVal_idx), axis=0)\n",
    "y_test = np.concatenate(np.take(label_by_seq, test_idx), axis=0)\n",
    "\n",
    "#Extract and concatenate the ultrasound images for train / test / val\n",
    "X_trainAndVal = np.concatenate(np.take(us_data_by_seq, trainAndVal_idx), axis=0)\n",
    "X_test = np.concatenate(np.take(us_data_by_seq, test_idx), axis=0)\n",
    "\n",
    "#Extract training and validation sets from trainAndVal using 80/20 split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainAndVal, y_trainAndVal, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of any nans\n",
    "y_train[pd.isnull(y_train)] = 'Undefined'\n",
    "y_val[pd.isnull(y_val)] = 'Undefined'\n",
    "y_test[pd.isnull(y_test)] = 'Undefined'\n",
    "\n",
    "#onehot encode the labels\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_val = encoder.fit_transform(y_val.reshape(-1,1)).toarray()\n",
    "y_test = encoder.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "# y_onehot = y_onehot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# y_onehot = pd.DataFrame(encoder.fit_transform())\n",
    "\n",
    "# X_trainANDval, X_test, y_trainANDval, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_trainANDval, y_trainANDval, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 13440 images, validated on 3361 images, tested on 6111 images.\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained on {} images, validated on {} images, tested on {} images.\".format(X_train.shape[0],\n",
    "                                                                                 X_val.shape[0],\n",
    "                                                                                 X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5adfbe80a24b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network architecture\n",
    "\n",
    "### Arch 1 - simple cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the unet model\n",
    "unet_model =  tf.keras.models.load_model(unet_fold_3, compile=False)\n",
    "unet_model.trainable = False\n",
    "\n",
    "def freezeWeights(model,num_customLayers):\n",
    "    numUntrainableLayers = len(model.layers) - num_customLayers\n",
    "    for layer in model.layers[:numUntrainableLayers]:\n",
    "        layer.trainable=False\n",
    "    for layer in model.layers[numUntrainableLayers:]:\n",
    "        layer.trainable=True\n",
    "    return model\n",
    "\n",
    "#Define the architecture of the classifier\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(128,128,3))\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "unet_layer = unet_model(inputs=input_layer)\n",
    "\n",
    "cov = Conv2D(8, (3, 3), activation='relu')(unet_layer)  \n",
    "cov = Conv2D(8, (3, 3), activation='relu')(cov)  \n",
    "pl = MaxPool2D((3, 3))(cov)   \n",
    "\n",
    "# cov = Conv2D(32, (3, 3), activation='relu')(pl)  \n",
    "# cov = Conv2D(32, (3, 3), activation='relu')(cov)  \n",
    "# pl = MaxPool2D((3, 3))(cov)\n",
    "\n",
    "# cov = Conv2D(64, (3, 3), activation='relu')(pl)  \n",
    "# pl = MaxPool2D((2, 2))(cov)\n",
    "\n",
    "fl = Flatten()(pl)\n",
    "\n",
    "# resize_layer = Conv2D(3,1,padding='same')(unet_layer)\n",
    "\n",
    "# mobilenet_layer = base_model(resize_layer)\n",
    "\n",
    "# avg_pool_layer = GlobalAveragePooling2D()(mobilenet_layer)\n",
    "\n",
    "d1 = Dense(64, activation='relu')(fl)\n",
    "\n",
    "d2 = Dense(3, activation='softmax')(d1)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=d2,\n",
    ")\n",
    "\n",
    "# for layer in model.layers[:-2]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 128, 128, 2)       304902    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 126, 126, 8)       152       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 124, 124, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 41, 41, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 13448)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                860736    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,166,569\n",
      "Trainable params: 861,667\n",
      "Non-trainable params: 304,902\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13440 samples, validate on 3361 samples\n",
      "Epoch 1/10\n",
      "13440/13440 [==============================] - 98s 7ms/sample - loss: 0.7000 - accuracy: 0.7852 - val_loss: 0.2015 - val_accuracy: 0.9390\n",
      "Epoch 2/10\n",
      "13440/13440 [==============================] - 98s 7ms/sample - loss: 0.1765 - accuracy: 0.9469 - val_loss: 0.1561 - val_accuracy: 0.9554\n",
      "Epoch 3/10\n",
      "13440/13440 [==============================] - 96s 7ms/sample - loss: 0.1560 - accuracy: 0.9556 - val_loss: 0.1464 - val_accuracy: 0.9634\n",
      "Epoch 4/10\n",
      "13440/13440 [==============================] - 97s 7ms/sample - loss: 0.1479 - accuracy: 0.9616 - val_loss: 0.1495 - val_accuracy: 0.9592\n",
      "Epoch 5/10\n",
      "13440/13440 [==============================] - 96s 7ms/sample - loss: 0.1407 - accuracy: 0.9636 - val_loss: 0.1340 - val_accuracy: 0.9661\n",
      "Epoch 6/10\n",
      "13440/13440 [==============================] - 96s 7ms/sample - loss: 0.1366 - accuracy: 0.9638 - val_loss: 0.1348 - val_accuracy: 0.9661\n",
      "Epoch 7/10\n",
      "13440/13440 [==============================] - 97s 7ms/sample - loss: 0.1279 - accuracy: 0.9669 - val_loss: 0.1291 - val_accuracy: 0.9694\n",
      "Epoch 8/10\n",
      "13440/13440 [==============================] - 96s 7ms/sample - loss: 0.1241 - accuracy: 0.9686 - val_loss: 0.1306 - val_accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "13440/13440 [==============================] - 97s 7ms/sample - loss: 0.1199 - accuracy: 0.9711 - val_loss: 0.1316 - val_accuracy: 0.9699\n",
      "Epoch 10/10\n",
      "13440/13440 [==============================] - 95s 7ms/sample - loss: 0.1190 - accuracy: 0.9717 - val_loss: 0.1249 - val_accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=300,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111/6111 [==============================] - 19s 3ms/sample - loss: 0.3852 - accuracy: 0.8683\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "test_pred_labels = tf.argmax(test_pred, axis=1)\n",
    "test_actual_labels = tf.argmax(y_test, axis=1)\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[2046,   90,  143],\n",
       "       [  47,  225,   12],\n",
       "       [  64,  449, 3035]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(test_actual_labels, test_pred_labels , 3)\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Cross-section       0.95      0.90      0.92      2279\n",
      "    Long-axis       0.29      0.79      0.43       284\n",
      "    Undefined       0.95      0.86      0.90      3548\n",
      "\n",
      "     accuracy                           0.87      6111\n",
      "    macro avg       0.73      0.85      0.75      6111\n",
      " weighted avg       0.92      0.87      0.89      6111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "target_names = ['Cross-section', 'Long-axis', 'Undefined']\n",
    "\n",
    "print(classification_report(test_actual_labels, test_pred_labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Data\\Logs\\CrossVal_Classifier\\CentralLine_ProbeOrientationClassifier_fold_4_2020-12-01_19-48-48\\assets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-caf5ad5253ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnotebook_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_fold_4_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msave_timestamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel_fullname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[1;32m--> 115\u001b[1;33m                           signatures, options)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# we use the default replica context here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       \u001b[0msave_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    921\u001b[0m       compat.as_str(constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0;32m    922\u001b[0m   object_graph_proto = _serialize_object_graph(\n\u001b[1;32m--> 923\u001b[1;33m       saveable_view, asset_info.asset_index)\n\u001b[0m\u001b[0;32m    924\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_graph_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_serialize_object_graph\u001b[1;34m(saveable_view, asset_file_def_index)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_proto\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaveable_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[0m_write_object_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_proto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masset_file_def_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36m_write_object_proto\u001b[1;34m(obj, proto, asset_file_def_index)\u001b[0m\n\u001b[0;32m    690\u001b[0m           version=versions_pb2.VersionDef(\n\u001b[0;32m    691\u001b[0m               producer=1, min_consumer=1, bad_consumers=[]),\n\u001b[1;32m--> 692\u001b[1;33m           metadata=obj._tracking_metadata)\n\u001b[0m\u001b[0;32m    693\u001b[0m       \u001b[1;31m# pylint:enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregistered_type_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_tracking_metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2410\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2411\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_tracking_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2412\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saved_model_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtracking_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2414\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_list_extra_dependencies_for_serialization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialization_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\base_serialization.py\u001b[0m in \u001b[0;36mtracking_metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# object is in the python property)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     return json.dumps(\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_properties\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         default=serialization.get_json_type)\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\layer_serialization.py\u001b[0m in \u001b[0;36mpython_properties\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpython_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# TODO(kathywu): Add python property validator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_properties_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_python_properties_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\model_serialization.py\u001b[0m in \u001b[0;36m_python_properties_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m     metadata.update(\n\u001b[0;32m     37\u001b[0m         saving_utils.model_metadata(\n\u001b[1;32m---> 38\u001b[1;33m             self.obj, include_optimizer=True, require_config=False))\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\repos\\dlenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[1;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m       metadata['training_config'] = {\n\u001b[1;32m--> 192\u001b[1;33m           \u001b[1;34m'loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m           \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m           \u001b[1;34m'metrics'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "output_dir = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Data\\Logs\\CrossVal_Classifier\"\n",
    "notebook_name = \"CentralLine_ProbeOrientationClassifier\"\n",
    "save_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# Archive trained model with unique filename based on notebook name and timestamp\n",
    "model_file_name = notebook_name + \"_fold_4_\" + save_timestamp\n",
    "model_fullname = os.path.join(output_dir, model_file_name)\n",
    "model.save(model_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures / Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9215500000000001 std 0.032250775184482015\n",
      "Precision: 0.9225 std 0.0178535710713571\n",
      "Recall: 0.8975 std 0.04437059837324713\n"
     ]
    }
   ],
   "source": [
    "acc = np.array([0.9422,0.9285,0.9484,0.8671])\n",
    "prec = np.array([0.90,0.92,0.95,0.92])\n",
    "rec = np.array([0.84,0.93,0.95,0.87])\n",
    "\n",
    "print(\"Accuracy: {} std {}\".format(acc.mean(), acc.std()))\n",
    "print(\"Precision: {} std {}\".format(prec.mean(), prec.std()))\n",
    "print(\"Recall: {} std {}\".format(rec.mean(), rec.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_confusion = np.array([[2421+1224+1932+2035, 114+95+6+100, 80+19+27+144],\n",
    "                              [33+216+194+41, 283+106+290+228, 88+42+35+15],\n",
    "                              [11+6+5+60, 9+21+13+452, 2756+3849+2925+3036]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 17\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.92863243 0.03842869 0.03293888]\n",
      " [0.30808402 0.57733927 0.1145767 ]\n",
      " [0.00623906 0.03766263 0.9560983 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues,\n",
    "                        save_fig = False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 3),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    if save_fig:\n",
    "        file_path = os.path.join(output_dir, \"Confusion_Matrix.pdf\")\n",
    "        plt.savefig(file_path, bbox_inches = \"tight\")\n",
    "        \n",
    "    \n",
    "# plot_confusion_matrix(confusion_matrix.numpy(), \n",
    "#                       [\"Cross-section\", \"Long-axis\", \"Undefined\"], \n",
    "#                       normalize=True,\n",
    "#                      title='Test Set Confusion Matrix',\n",
    "#                      save_fig=True)\n",
    "\n",
    "plot_confusion_matrix(overall_confusion, \n",
    "                      [\"Cross-section\", \"Long-axis\", \"Undefined\"], \n",
    "                      normalize=True,\n",
    "                     title='Test Set Confusion Matrix',\n",
    "                     save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network architecture\n",
    "\n",
    "### Arch 1 - with mobile net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the unet model\n",
    "unet_model =  tf.keras.models.load_model(unet_path, compile=False)\n",
    "unet_model.trainable = False\n",
    "\n",
    "def freezeWeights(model,num_customLayers):\n",
    "    numUntrainableLayers = len(model.layers) - num_customLayers\n",
    "    for layer in model.layers[:numUntrainableLayers]:\n",
    "        layer.trainable=False\n",
    "    for layer in model.layers[numUntrainableLayers:]:\n",
    "        layer.trainable=True\n",
    "    return model\n",
    "\n",
    "#Define the architecture of the classifier\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(128,128,3))\n",
    "\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "unet_layer = unet_model(inputs=input_layer)\n",
    "\n",
    "cov = Conv2D(8, (3, 3), activation='relu')(unet_layer)  \n",
    "cov = Conv2D(8, (3, 3), activation='relu')(cov)  \n",
    "pl = MaxPool2D((3, 3))(cov)   \n",
    "\n",
    "cov = Conv2D(16, (3, 3), activation='relu')(pl)  \n",
    "cov = Conv2D(16, (3, 3), activation='relu')(cov)  \n",
    "pl = MaxPool2D((3, 3))(cov)\n",
    "\n",
    "# cov = Conv2D(64, (3, 3), activation='relu')(pl)  \n",
    "# pl = MaxPool2D((2, 2))(cov)\n",
    "\n",
    "fl = Flatten()(pl)\n",
    "\n",
    "# resize_layer = Conv2D(3,1,padding='same')(unet_layer)\n",
    "\n",
    "# mobilenet_layer = base_model(resize_layer)\n",
    "\n",
    "# avg_pool_layer = GlobalAveragePooling2D()(mobilenet_layer)\n",
    "\n",
    "d1 = Dense(32, activation='relu')(fl)\n",
    "\n",
    "d2 = Dense(3, activation='softmax')(d1)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=d2,\n",
    ")\n",
    "\n",
    "# for layer in model.layers[:-2]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6873 samples, validate on 2291 samples\n",
      "Epoch 1/10\n",
      "6873/6873 [==============================] - 59s 9ms/sample - loss: 0.4491 - accuracy: 0.8627 - val_loss: 0.1920 - val_accuracy: 0.9502\n",
      "Epoch 2/10\n",
      "6873/6873 [==============================] - 54s 8ms/sample - loss: 0.1980 - accuracy: 0.9428 - val_loss: 0.1583 - val_accuracy: 0.9559\n",
      "Epoch 3/10\n",
      "6873/6873 [==============================] - 53s 8ms/sample - loss: 0.1717 - accuracy: 0.9494 - val_loss: 0.1485 - val_accuracy: 0.9568\n",
      "Epoch 4/10\n",
      "6873/6873 [==============================] - 53s 8ms/sample - loss: 0.1619 - accuracy: 0.9556 - val_loss: 0.1409 - val_accuracy: 0.9633\n",
      "Epoch 5/10\n",
      "6873/6873 [==============================] - 54s 8ms/sample - loss: 0.1467 - accuracy: 0.9609 - val_loss: 0.1313 - val_accuracy: 0.9660\n",
      "Epoch 6/10\n",
      "6873/6873 [==============================] - 54s 8ms/sample - loss: 0.1344 - accuracy: 0.9629 - val_loss: 0.1218 - val_accuracy: 0.9664\n",
      "Epoch 7/10\n",
      "6873/6873 [==============================] - 55s 8ms/sample - loss: 0.1300 - accuracy: 0.9639 - val_loss: 0.1331 - val_accuracy: 0.9629\n",
      "Epoch 8/10\n",
      "6873/6873 [==============================] - 53s 8ms/sample - loss: 0.1236 - accuracy: 0.9667 - val_loss: 0.1147 - val_accuracy: 0.9694\n",
      "Epoch 9/10\n",
      "6873/6873 [==============================] - 53s 8ms/sample - loss: 0.1112 - accuracy: 0.9697 - val_loss: 0.1106 - val_accuracy: 0.9703\n",
      "Epoch 10/10\n",
      "6873/6873 [==============================] - 53s 8ms/sample - loss: 0.1067 - accuracy: 0.9722 - val_loss: 0.1096 - val_accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, \n",
    "                    y=y_train, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=300,\n",
    "                    epochs=10,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13748/13748 [==============================] - 44s 3ms/sample - loss: 0.1070 - accuracy: 0.9724\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "test_pred_labels = np.array(tf.argmax(test_pred, axis=1))\n",
    "test_actual_labels = np.array(tf.argmax(y_test, axis=1))\n",
    "num_classes = 3\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(test_actual_labels, test_pred_labels , 3)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "model (Model)                (None, 128, 128, 2)       304902    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 128, 128, 3)       6         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_128 (Model) (None, 4, 4, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_15  (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 256)               327936    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 2,891,599\n",
      "Trainable params: 328,707\n",
      "Non-trainable params: 2,562,892\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "unet_layer = unet_model(inputs=input_layer)\n",
    "\n",
    "slice_layer = Lambda(lambda x: x[:,:,:,1][...,np.newaxis])(unet_layer)\n",
    "\n",
    "resize_layer = Conv2D(3,1,padding='same')(slice_layer)\n",
    "\n",
    "mobilenet_layer = base_model(resize_layer)\n",
    "\n",
    "avg_pool_layer = GlobalAveragePooling2D()(mobilenet_layer)\n",
    "\n",
    "d1 = Dense(256, activation='relu')(avg_pool_layer)\n",
    "\n",
    "d2 = Dense(3, activation='softmax')(d1)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=d2,\n",
    ")\n",
    "\n",
    "model = freezeWeights(model,2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "testing_df = np.random.rand(20,128,128,3)\n",
    "predict_df = np.random.randint(1,2, size=(20,128,128))\n",
    "\n",
    "predict_df_expanded = np.repeat(predict_df[...,np.newaxis], 3, axis=3)\n",
    "predict_inverse_expanded = np.logical_not(predict_df_expanded)\n",
    "masked_testing_df = np.ma.masked_array(testing_df, predict_inverse_expanded, fill_value = 0).filled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[8.35877153e-01, 5.41836905e-01, 9.91461218e-01],\n",
       "         [2.88878922e-02, 2.54884085e-01, 5.31910137e-01],\n",
       "         [9.25983140e-01, 6.52142377e-01, 2.44723308e-01],\n",
       "         ...,\n",
       "         [3.57584478e-01, 1.32230900e-01, 2.27174824e-01],\n",
       "         [8.22208348e-02, 6.06720405e-02, 4.70859612e-01],\n",
       "         [3.74623330e-01, 9.84624131e-01, 4.88887309e-01]],\n",
       "\n",
       "        [[5.82963516e-01, 5.60212655e-01, 3.21740623e-01],\n",
       "         [1.39595523e-03, 9.90178588e-02, 1.53252572e-01],\n",
       "         [9.46720839e-01, 3.49501989e-01, 6.95037753e-01],\n",
       "         ...,\n",
       "         [4.67403473e-01, 5.76413283e-01, 5.08869957e-01],\n",
       "         [9.65769266e-01, 8.27067252e-01, 4.52992714e-01],\n",
       "         [3.04168134e-01, 2.91634401e-01, 7.23204263e-02]],\n",
       "\n",
       "        [[7.96636771e-01, 4.15745417e-01, 9.33040574e-01],\n",
       "         [8.37693409e-01, 9.20920302e-01, 4.87073660e-01],\n",
       "         [1.04629512e-01, 5.40882998e-02, 5.86317313e-01],\n",
       "         ...,\n",
       "         [5.43061834e-01, 8.34516387e-01, 8.70921879e-01],\n",
       "         [2.90656097e-03, 4.09844610e-01, 2.03665372e-01],\n",
       "         [5.17003259e-01, 6.88738827e-01, 5.28368133e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.90666794e-01, 9.89374479e-01, 2.67261635e-01],\n",
       "         [6.14045604e-01, 5.46187935e-01, 6.93206170e-01],\n",
       "         [2.70657257e-01, 3.24417263e-01, 3.42009519e-01],\n",
       "         ...,\n",
       "         [9.45117848e-01, 6.87842200e-01, 5.34561432e-01],\n",
       "         [1.68564713e-01, 3.69851650e-01, 4.86756317e-01],\n",
       "         [8.26617909e-01, 9.54526141e-01, 1.33003377e-02]],\n",
       "\n",
       "        [[4.79721908e-01, 9.19805604e-01, 1.79912628e-02],\n",
       "         [1.29759432e-01, 6.47940915e-01, 2.01483101e-01],\n",
       "         [7.69545129e-01, 8.29941692e-01, 9.55143290e-02],\n",
       "         ...,\n",
       "         [4.42750544e-02, 1.58684317e-01, 3.66997865e-01],\n",
       "         [6.89548413e-01, 3.01926573e-02, 1.42544419e-01],\n",
       "         [9.70591784e-01, 9.49286007e-02, 1.88343247e-01]],\n",
       "\n",
       "        [[4.44771025e-04, 5.63795022e-01, 2.06977894e-01],\n",
       "         [9.10995348e-01, 5.67806313e-01, 3.55646573e-01],\n",
       "         [3.83480602e-01, 8.81095298e-01, 3.48935631e-01],\n",
       "         ...,\n",
       "         [7.22935001e-01, 9.67397506e-01, 3.11810608e-01],\n",
       "         [7.45524627e-01, 2.75282712e-01, 1.91976528e-01],\n",
       "         [5.04992759e-01, 3.45785508e-01, 3.79227312e-01]]],\n",
       "\n",
       "\n",
       "       [[[2.28584093e-01, 6.43634004e-01, 8.70340417e-01],\n",
       "         [2.08261485e-02, 8.78688942e-01, 3.03077761e-01],\n",
       "         [3.88450708e-01, 3.58260662e-01, 3.49749652e-01],\n",
       "         ...,\n",
       "         [7.88554749e-01, 6.89479037e-02, 7.13976193e-01],\n",
       "         [6.88603984e-01, 5.62677901e-01, 5.96621137e-01],\n",
       "         [6.64122983e-01, 1.98747473e-01, 9.17076809e-01]],\n",
       "\n",
       "        [[6.88383559e-01, 5.31922350e-01, 4.98549895e-01],\n",
       "         [4.12554508e-01, 6.49846197e-02, 6.88366048e-01],\n",
       "         [9.48991199e-01, 2.02518924e-01, 2.38468119e-01],\n",
       "         ...,\n",
       "         [5.93405835e-01, 2.50617640e-01, 8.49464794e-01],\n",
       "         [7.44162283e-03, 6.09992803e-01, 3.25920491e-01],\n",
       "         [1.08437989e-01, 2.30443050e-01, 1.86030601e-01]],\n",
       "\n",
       "        [[5.17887693e-01, 9.58121726e-02, 3.28309867e-01],\n",
       "         [5.12030600e-01, 6.63727492e-01, 9.48110506e-01],\n",
       "         [6.13025006e-02, 3.74140665e-01, 2.46466815e-01],\n",
       "         ...,\n",
       "         [6.09254245e-01, 7.11719047e-01, 7.42189813e-01],\n",
       "         [3.06153185e-01, 7.36228325e-01, 6.07372440e-01],\n",
       "         [6.42538554e-01, 9.96696291e-01, 9.91430544e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[8.27803049e-01, 7.92026578e-01, 2.01287544e-01],\n",
       "         [5.03397553e-01, 6.28706413e-01, 1.25727275e-01],\n",
       "         [9.38380674e-01, 7.28939827e-02, 5.18387083e-01],\n",
       "         ...,\n",
       "         [2.83133644e-01, 6.58455595e-02, 2.13057187e-01],\n",
       "         [3.26852137e-01, 8.69709880e-01, 8.05246369e-01],\n",
       "         [5.65332245e-01, 5.31071789e-03, 5.09202029e-01]],\n",
       "\n",
       "        [[9.57053902e-01, 6.11410337e-01, 6.02205028e-01],\n",
       "         [6.81456590e-01, 8.25272756e-01, 7.26052273e-01],\n",
       "         [2.91700000e-01, 1.69506751e-01, 7.32967004e-01],\n",
       "         ...,\n",
       "         [1.38586907e-01, 9.01835694e-01, 2.38834296e-01],\n",
       "         [4.83676399e-01, 2.85096601e-01, 8.31949090e-01],\n",
       "         [1.74943017e-01, 7.84049866e-01, 4.24747469e-01]],\n",
       "\n",
       "        [[6.34850981e-01, 2.68711412e-01, 8.65942674e-01],\n",
       "         [7.00175560e-01, 5.49724286e-01, 4.16345877e-01],\n",
       "         [9.09499266e-01, 2.71711205e-01, 5.11497440e-01],\n",
       "         ...,\n",
       "         [1.09775966e-02, 2.82870508e-01, 3.10475066e-01],\n",
       "         [8.71942184e-01, 6.69739799e-01, 7.06068690e-01],\n",
       "         [8.40536925e-01, 2.69281692e-01, 5.66470502e-01]]],\n",
       "\n",
       "\n",
       "       [[[6.94040167e-01, 4.03525581e-01, 5.30708644e-01],\n",
       "         [2.05319326e-01, 8.88130967e-01, 7.21925615e-01],\n",
       "         [6.64983541e-01, 8.26451673e-01, 6.47788369e-01],\n",
       "         ...,\n",
       "         [4.09960495e-01, 9.17467753e-01, 2.09263450e-01],\n",
       "         [7.96662913e-01, 3.46397633e-01, 1.13329923e-01],\n",
       "         [7.59778483e-01, 4.84572751e-02, 1.96255265e-01]],\n",
       "\n",
       "        [[5.76084528e-02, 7.43740814e-01, 4.13041396e-01],\n",
       "         [6.22178029e-01, 7.47678117e-01, 9.07986729e-01],\n",
       "         [2.32205084e-01, 9.83628184e-02, 9.60861651e-01],\n",
       "         ...,\n",
       "         [6.45950452e-01, 6.03603804e-01, 4.39316989e-01],\n",
       "         [8.45258591e-02, 2.07160038e-01, 5.14827543e-01],\n",
       "         [5.91398825e-01, 3.70661676e-01, 2.74786521e-01]],\n",
       "\n",
       "        [[3.56177263e-01, 8.41378622e-01, 2.18109707e-01],\n",
       "         [9.89840045e-01, 5.55300094e-02, 8.39928313e-01],\n",
       "         [9.03181262e-01, 4.90900836e-01, 4.73740579e-01],\n",
       "         ...,\n",
       "         [8.12673249e-01, 3.74209636e-01, 6.98568000e-01],\n",
       "         [6.52475666e-01, 2.70270532e-01, 5.59987211e-01],\n",
       "         [8.59012535e-01, 8.29460621e-01, 8.86742090e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[8.34115960e-01, 1.37073165e-01, 6.55599219e-02],\n",
       "         [1.35631388e-01, 8.67668118e-01, 5.75652427e-01],\n",
       "         [9.59623862e-01, 2.81222080e-01, 3.59899241e-01],\n",
       "         ...,\n",
       "         [9.94310720e-01, 7.74222462e-01, 8.08255251e-01],\n",
       "         [4.02142970e-01, 5.93647468e-02, 5.81352054e-01],\n",
       "         [5.26055806e-01, 2.82295855e-01, 4.90382865e-01]],\n",
       "\n",
       "        [[9.35572770e-01, 4.87343504e-01, 9.42115122e-01],\n",
       "         [3.03807470e-01, 3.60364676e-01, 9.88574730e-01],\n",
       "         [1.32242056e-04, 2.95975944e-01, 4.38588225e-01],\n",
       "         ...,\n",
       "         [4.62384757e-01, 6.10765181e-01, 9.83524877e-01],\n",
       "         [7.83000545e-02, 3.19632223e-01, 8.73868398e-02],\n",
       "         [1.48090261e-01, 7.26340657e-01, 2.71732119e-01]],\n",
       "\n",
       "        [[2.97054928e-02, 4.93926391e-01, 7.97509902e-02],\n",
       "         [1.00762062e-01, 1.32521643e-01, 4.10984352e-01],\n",
       "         [2.01361685e-01, 3.83335188e-01, 2.80238378e-01],\n",
       "         ...,\n",
       "         [2.92245773e-01, 5.05681984e-01, 6.80423350e-02],\n",
       "         [5.21590960e-01, 3.07713920e-01, 9.77863764e-01],\n",
       "         [7.16866156e-01, 3.52457795e-01, 7.84751343e-01]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[7.82467645e-01, 8.49490941e-01, 5.65861593e-02],\n",
       "         [3.96714923e-01, 9.50145193e-01, 9.53432640e-01],\n",
       "         [6.65273690e-01, 6.09609897e-02, 8.48164962e-01],\n",
       "         ...,\n",
       "         [8.23045065e-01, 6.28585671e-01, 7.15045316e-03],\n",
       "         [7.47450639e-01, 1.81506593e-01, 6.60348784e-01],\n",
       "         [1.39001435e-01, 8.21989998e-01, 7.91668638e-02]],\n",
       "\n",
       "        [[2.76222363e-01, 8.58317167e-01, 3.56421876e-01],\n",
       "         [2.52373666e-01, 6.93879744e-01, 2.22970579e-01],\n",
       "         [6.25698669e-01, 7.06795924e-01, 5.02357745e-01],\n",
       "         ...,\n",
       "         [2.33614066e-01, 2.96821243e-01, 5.62014703e-01],\n",
       "         [3.00740477e-01, 1.18002495e-01, 6.73161018e-01],\n",
       "         [3.10579718e-01, 1.24267193e-01, 1.23529607e-01]],\n",
       "\n",
       "        [[7.37066632e-01, 9.68429102e-01, 6.74823190e-01],\n",
       "         [6.07786211e-01, 6.49482354e-01, 2.97155652e-01],\n",
       "         [6.01272526e-01, 3.89473494e-01, 2.98290761e-01],\n",
       "         ...,\n",
       "         [7.52172365e-01, 2.02943233e-01, 5.09615356e-01],\n",
       "         [1.44543361e-01, 4.77194326e-01, 4.13810800e-01],\n",
       "         [5.26552910e-01, 1.82698261e-01, 7.94641753e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.46339035e-01, 6.64029508e-01, 8.69505789e-01],\n",
       "         [2.10617618e-01, 8.82037334e-01, 9.67797398e-01],\n",
       "         [7.08823905e-01, 8.93542786e-01, 8.41039252e-01],\n",
       "         ...,\n",
       "         [6.22911200e-02, 7.33690601e-01, 4.10629726e-01],\n",
       "         [1.23058817e-01, 7.23648681e-01, 3.46152188e-01],\n",
       "         [1.91850444e-01, 8.49994777e-02, 9.80081212e-02]],\n",
       "\n",
       "        [[7.21544406e-01, 9.35215858e-01, 5.81999284e-01],\n",
       "         [8.46747366e-01, 1.47714908e-01, 4.67749147e-01],\n",
       "         [4.36313316e-01, 1.34097093e-01, 9.92490151e-01],\n",
       "         ...,\n",
       "         [5.00241050e-01, 6.35887093e-01, 8.00588737e-01],\n",
       "         [2.39789267e-01, 4.79750606e-01, 6.17127439e-01],\n",
       "         [7.73725640e-01, 7.08069041e-01, 5.76262102e-01]],\n",
       "\n",
       "        [[5.71221114e-01, 7.63449757e-01, 6.18656384e-01],\n",
       "         [5.70615903e-01, 4.77353351e-01, 6.85934463e-01],\n",
       "         [3.09472812e-01, 6.65894067e-01, 8.03637010e-01],\n",
       "         ...,\n",
       "         [6.35625973e-01, 5.62708252e-01, 4.74666064e-01],\n",
       "         [4.70040279e-01, 9.22115336e-02, 6.93708446e-01],\n",
       "         [1.34996813e-01, 6.00764464e-02, 3.88647277e-01]]],\n",
       "\n",
       "\n",
       "       [[[9.97563063e-01, 5.58796406e-02, 8.62896203e-01],\n",
       "         [9.72560785e-01, 5.20016121e-01, 7.74769900e-01],\n",
       "         [5.21814415e-01, 1.16246008e-01, 4.13219857e-01],\n",
       "         ...,\n",
       "         [5.24090216e-02, 2.66211313e-01, 9.03457973e-02],\n",
       "         [4.81851450e-01, 9.13739056e-01, 2.62300665e-01],\n",
       "         [6.29903548e-01, 4.95660656e-01, 1.10852140e-02]],\n",
       "\n",
       "        [[8.21265923e-01, 3.66078584e-01, 9.21182651e-01],\n",
       "         [1.83144905e-01, 3.51657309e-01, 6.03528725e-01],\n",
       "         [7.04962789e-01, 5.93485632e-01, 8.19089894e-01],\n",
       "         ...,\n",
       "         [3.00196183e-01, 5.52213294e-02, 1.09849659e-01],\n",
       "         [3.85070382e-01, 8.33901629e-01, 9.29665998e-01],\n",
       "         [8.52505619e-01, 1.44606901e-01, 1.19651241e-01]],\n",
       "\n",
       "        [[1.84668736e-01, 3.27025618e-01, 9.15125860e-01],\n",
       "         [7.42191525e-01, 1.58592949e-01, 9.40728595e-01],\n",
       "         [2.15485863e-01, 9.21511340e-01, 2.70276217e-01],\n",
       "         ...,\n",
       "         [3.35650049e-01, 2.93459572e-01, 4.09259578e-01],\n",
       "         [6.88523980e-01, 1.64620569e-01, 9.22056355e-01],\n",
       "         [2.32090780e-02, 8.82873994e-01, 9.80176058e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2.05997124e-01, 9.42868406e-01, 1.56741103e-01],\n",
       "         [9.60307600e-01, 8.14944983e-02, 1.80746415e-02],\n",
       "         [7.60326424e-01, 3.51244417e-01, 6.36318589e-01],\n",
       "         ...,\n",
       "         [8.51987202e-01, 5.39059766e-01, 7.64369493e-01],\n",
       "         [6.96424840e-01, 3.50553572e-01, 3.86824077e-01],\n",
       "         [1.76230610e-01, 6.35507324e-01, 5.53432334e-01]],\n",
       "\n",
       "        [[2.81956324e-01, 2.82099047e-01, 2.92747718e-01],\n",
       "         [8.24593999e-01, 2.39844355e-01, 4.23066894e-01],\n",
       "         [4.62701631e-01, 5.90360561e-01, 8.77821315e-01],\n",
       "         ...,\n",
       "         [2.66793329e-01, 6.70786454e-01, 4.52816230e-01],\n",
       "         [6.98440684e-01, 7.84815886e-01, 4.94206815e-01],\n",
       "         [1.99938732e-01, 9.55568958e-01, 7.27634436e-01]],\n",
       "\n",
       "        [[3.95360968e-01, 9.84365626e-01, 4.82829868e-02],\n",
       "         [7.69689451e-01, 8.15312928e-01, 6.93032444e-02],\n",
       "         [4.00667672e-01, 8.03712074e-01, 2.91178773e-01],\n",
       "         ...,\n",
       "         [5.94720419e-01, 5.48959819e-01, 8.06506594e-01],\n",
       "         [8.35398724e-01, 7.94822383e-01, 3.80466511e-01],\n",
       "         [5.25198102e-01, 5.44573729e-01, 4.18707343e-01]]],\n",
       "\n",
       "\n",
       "       [[[1.83835067e-02, 5.37205945e-01, 2.88558963e-01],\n",
       "         [8.24931286e-01, 3.06723133e-01, 6.26890646e-01],\n",
       "         [1.77614722e-01, 7.20929566e-01, 4.17246737e-01],\n",
       "         ...,\n",
       "         [2.61486990e-01, 2.82097855e-01, 6.89320843e-01],\n",
       "         [5.33849704e-01, 3.10896377e-01, 6.48190382e-02],\n",
       "         [1.13266093e-01, 7.01529815e-01, 2.95894262e-01]],\n",
       "\n",
       "        [[6.01791727e-01, 5.29451528e-01, 9.11780489e-02],\n",
       "         [2.66389865e-01, 5.71832363e-01, 3.84510939e-01],\n",
       "         [5.24690659e-01, 6.88525580e-01, 4.01355002e-01],\n",
       "         ...,\n",
       "         [6.56245882e-01, 9.97601394e-01, 4.91209363e-01],\n",
       "         [4.03334603e-01, 7.35450030e-01, 1.87603004e-01],\n",
       "         [4.64999057e-01, 1.11997628e-02, 1.13417076e-01]],\n",
       "\n",
       "        [[7.92991637e-01, 1.15665077e-01, 3.79367095e-01],\n",
       "         [9.78878046e-01, 8.57382328e-02, 4.91502311e-01],\n",
       "         [9.15989491e-01, 6.92036162e-01, 4.90933474e-01],\n",
       "         ...,\n",
       "         [7.57787701e-01, 2.02907499e-01, 2.55758140e-01],\n",
       "         [4.73790721e-02, 8.72587213e-02, 3.90211657e-01],\n",
       "         [6.09039284e-01, 5.62702050e-01, 1.56965171e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[7.44604068e-01, 2.33033051e-01, 4.79039861e-01],\n",
       "         [5.46971485e-01, 5.63133859e-01, 6.04663898e-01],\n",
       "         [2.83820680e-01, 8.95394819e-01, 1.85142278e-01],\n",
       "         ...,\n",
       "         [8.33026763e-01, 4.23299274e-01, 6.18364950e-01],\n",
       "         [3.26146067e-01, 8.86281726e-01, 1.83560865e-02],\n",
       "         [9.55185439e-01, 7.76169522e-01, 1.82398700e-01]],\n",
       "\n",
       "        [[1.20920809e-01, 3.15723480e-01, 2.64779576e-01],\n",
       "         [7.26385098e-01, 5.88818199e-01, 4.80356589e-01],\n",
       "         [2.83965401e-01, 9.62573974e-01, 3.23912951e-01],\n",
       "         ...,\n",
       "         [9.40962110e-01, 5.89508317e-01, 5.13776457e-01],\n",
       "         [3.47990780e-01, 3.83623676e-01, 5.26243276e-01],\n",
       "         [9.12831485e-01, 6.35204174e-01, 4.76953095e-01]],\n",
       "\n",
       "        [[4.60568448e-01, 5.38399979e-01, 5.99018383e-01],\n",
       "         [7.97985864e-01, 8.39659234e-01, 9.28420881e-01],\n",
       "         [3.81508573e-01, 8.67019112e-02, 2.97960488e-01],\n",
       "         ...,\n",
       "         [6.71554255e-01, 4.88538204e-01, 2.51743911e-01],\n",
       "         [1.98665360e-01, 2.77264513e-01, 2.21127704e-01],\n",
       "         [3.69584507e-01, 8.85053540e-01, 8.84864678e-01]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_testing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlenv] *",
   "language": "python",
   "name": "conda-env-dlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
