{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Line UNet\n",
    "\n",
    "\n",
    "This noteboook contains all the code neccesary to train the UNet used in this project for ultrasound image segmentation.\n",
    "\n",
    "Note that it assumes [SlicerIGT/aigt](https://github.com/SlicerIGT/aigt) is already cloned locally.\n",
    "\n",
    "Things that must be changed to run this locally:\n",
    "- Output_dir\n",
    "- aigt_repo_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Define user parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Paths\n",
    "data_input_dir = r\"C:\\\\Users\\\\cbarr\\\\OneDrive - Queen's University\\\\Grad School\\\\Courses\\\\CISC 881\\\\Project\\\\Data\\\\PilotNpData\"\n",
    "aigt_repo_path = r\"C:\\repos\\aigt\"\n",
    "output_dir = r\"C:\\Users\\cbarr\\OneDrive - Queen's University\\Grad School\\Courses\\CISC 881\\Project\\Testing_Folder\"\n",
    "\n",
    "#Notebook name\n",
    "notebook_name = 'Central_Line_UNet'\n",
    "\n",
    "#Sequences to be used for validation and testing; the rest is for training\n",
    "test_idx = [0,1,2,3,4]\n",
    "current_fold = \"fold_1\"\n",
    "\n",
    "# Learning parameters\n",
    "ultrasound_size = 128                  #Dimensions of the US images used\n",
    "num_epochs = 1                         #Number of times to adjust weights following use of a batch of data\n",
    "batch_size = 128                       #Number of images to feed in for each epoch\n",
    "max_learning_rate = 0.02               #Used in defining Adam parameters\n",
    "min_learning_rate = 0.00001            #Used to generate learning_rate_decay\n",
    "regularization_rate = 0.0001           #Used in uNet definition; L1 bias regulation\n",
    "filter_multiplier = 8                  #used in UNet definition\n",
    "class_weights = np.array([0.1, 0.9])   #Weights for weighted categorical cross entropy\n",
    "learning_rate_decay = (max_learning_rate - min_learning_rate) / num_epochs #the rate at which the learning rate decays; adam parameter\n",
    "\n",
    "# Training data augmentation parameters for segmentation batch generation\n",
    "max_shift_factor = 0.12\n",
    "max_rotation_angle = 10\n",
    "max_zoom_factor = 1.1\n",
    "min_zoom_factor = 0.8\n",
    "\n",
    "acceptable_margin_mm = 1.0 #For evaluation of segmentation\n",
    "mm_per_pixel = 1.0\n",
    "\n",
    "#Define roc thresholds\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Imports\n",
    "## Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries in aigt repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(aigt_repo_path)\n",
    "sys.path.append(os.path.join(aigt_repo_path, \"UltrasoundSegmentation\"))\n",
    "\n",
    "import ultrasound_batch_generator as generator\n",
    "import evaluation_metrics\n",
    "import Models.segmentation_unet as unet\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the US image and segmentations paths into lists\n",
    "seg_files = list(sorted(Path(data_input_dir).glob(\"*_segmentation.npy\")))\n",
    "us_files = list(sorted(Path(data_input_dir).glob(\"*_ultrasound.npy\")))\n",
    "\n",
    "#Go through all file paths in both arrays and append to list\n",
    "seg_data_by_seq = []\n",
    "us_data_by_seq = []\n",
    "for i in range(len(seg_files)):\n",
    "\n",
    "    #Load the current files as 3D numpy arrays\n",
    "    seg_np = np.load(os.path.abspath(seg_files[i]))\n",
    "    us_np = np.load(os.path.abspath(us_files[i]))\n",
    "    \n",
    "    #Normalize and add channel dimension\n",
    "    seg_np = seg_np[...,np.newaxis] / 255\n",
    "    us_np = us_np[...,np.newaxis] / 255\n",
    "    \n",
    "    #Append to the collector lists.\n",
    "    seg_data_by_seq.append(seg_np.transpose(2,1,0,3))\n",
    "    us_data_by_seq.append(us_np.transpose(2,1,0,3))\n",
    "\n",
    "#Verify that the total number of segmentation images matches the total number of us images.\n",
    "for idx in range(len(seg_data_by_seq)):\n",
    "    if len(seg_data_by_seq[idx][0]) != len(us_data_by_seq[idx][0]):\n",
    "        print(\"Data Problem: Dataset {} has {} ultrasounds and {} segmentations\". format(\n",
    "            idx, len(seg_data_by_seq[idx][0]), len(us_data_by_seq[idx][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Divide data into training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of indices for training and validation sets\n",
    "trainAndVal_idx = list(range(len(seg_data_by_seq)))\n",
    "trainAndVal_idx = [x for x in trainAndVal_idx if x not in test_idx] #Remove IDs\n",
    "\n",
    "#Extract and concatenate the segmentations for train / test / val\n",
    "seg_trainAndVal = np.concatenate(np.take(seg_data_by_seq, trainAndVal_idx), axis=0)\n",
    "seg_test = np.concatenate(np.take(seg_data_by_seq, test_idx), axis=0)\n",
    "\n",
    "#Extract and concatenate the ultrasound images for train / test / val\n",
    "us_trainAndVal = np.concatenate(np.take(us_data_by_seq, trainAndVal_idx), axis=0)\n",
    "us_test = np.concatenate(np.take(us_data_by_seq, test_idx), axis=0)\n",
    "\n",
    "#Extract training and validation sets from trainAndVal using 80/20 split\n",
    "us_train, us_val, seg_train, seg_val = train_test_split(us_trainAndVal, seg_trainAndVal, test_size=0.2, random_state=2)\n",
    "\n",
    "#Convert validation categorical data to onehot dictated by the number of classes\n",
    "seg_val_onehot = tf.keras.utils.to_categorical(seg_val, 2)\n",
    "seg_train_onehot = tf.keras.utils.to_categorical(seg_train, 2)\n",
    "seg_test_onehot = tf.keras.utils.to_categorical(seg_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trained on {} images, validated on {} images, tested on {} images.\".format(us_train.shape[0],\n",
    "                                                                                 us_val.shape[0],\n",
    "                                                                                 us_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Define IoU loss and metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def IoU_loss(y_true,y_pred):\n",
    "    smooth = 1e-12\n",
    "    intersection = K.sum(y_true[:,:,:,1] * y_pred[:,:,:,1])        #Create intersection\n",
    "    sum_ = K.sum(y_true[:,:,:,1] + y_pred[:,:,:,1])                #Create union\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth) #Divide and smooth\n",
    "    return K.mean(1-jac) #Return 1-IoU so it can be use as a measurement of loss\n",
    "\n",
    "def IoU(y_true,y_pred):\n",
    "    smooth = 1e-12\n",
    "    y_pred_pos = K.round(K.clip(y_pred[:,:,:,1], 0, 1))             #Extract binary mask from probability map\n",
    "    intersection = K.sum(y_true[:,:,:,1] * y_pred_pos)              #Create union\n",
    "    sum_ = K.sum(y_true[:,:,:,1] + y_pred[:,:,:,1])                 #Create intersection\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)  #Divide and smooth\n",
    "    return K.mean(jac) #Return the mean jaccard index as IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Setup and compile U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the model object from the aigt unet\n",
    "model = unet.segmentation_unet(ultrasound_size, 2, filter_multiplier, regularization_rate)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=max_learning_rate, decay=learning_rate_decay),\n",
    "    loss=IoU_loss,\n",
    "    metrics=[IoU]\n",
    ")\n",
    "#Initialize the training generator\n",
    "training_generator = generator.UltrasoundSegmentationBatchGenerator(\n",
    "    us_train,                               #Training US images\n",
    "    seg_train[:, :, :, 0],                  #Background segmentation of labels\n",
    "    batch_size,                             #Batch size\n",
    "    (ultrasound_size, ultrasound_size),     #Image size\n",
    "    max_shift_factor=max_shift_factor,      #Properties for data augmentation\n",
    "    min_zoom_factor=min_zoom_factor,\n",
    "    max_zoom_factor=max_zoom_factor,\n",
    "    max_rotation_angle=max_rotation_angle\n",
    ")\n",
    "\n",
    "#Create a new timestamp for save files\n",
    "save_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Run the UNet and record results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record the time that training starts\n",
    "training_time_start = datetime.datetime.now()\n",
    "\n",
    "#Fit the generator to the model\n",
    "training_log = model.fit_generator(\n",
    "    training_generator,\n",
    "    validation_data=(us_val, seg_val_onehot),\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, )\n",
    "    ])\n",
    "\n",
    "#Record the time that training stops\n",
    "training_time_stop = datetime.datetime.now()\n",
    "\n",
    "#Print training time\n",
    "print(\"  Training time: {}\".format(training_time_stop-training_time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archive trained model with unique filename based on notebook name and timestamp\n",
    "model_file_name = notebook_name + current_fold + save_timestamp\n",
    "model_fullname = os.path.join(output_dir, model_file_name)\n",
    "model.save(model_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Graph the Loss and IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingStats(training_history):\n",
    "    # Plot the loss function\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "    ax.plot(training_history.history['loss'], 'r', label='train')\n",
    "    ax.plot(training_history.history['val_loss'], 'b' ,label='val')\n",
    "    ax.set_title(\"Loss VS Epoch Number\")\n",
    "    ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "    ax.set_ylabel(r'Loss', fontsize=20)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize=20)\n",
    "\n",
    "    # Plot the accuracy\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "    ax.plot(training_history.history['IoU'], 'r', label='train')\n",
    "    ax.plot(training_history.history['val_IoU'], 'b' ,label='val')\n",
    "    ax.set_title(\"IoU VS Epoch Number\")\n",
    "    ax.set_xlabel(r'Epoch', fontsize=20)\n",
    "    ax.set_ylabel(r'IoU', fontsize=20)\n",
    "    ax.legend()\n",
    "    ax.tick_params(labelsize=20)\n",
    "    \n",
    "plotTrainingStats(training_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Predict on Test set and print IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on the test set\n",
    "y_pred_test  = model.predict(us_test)\n",
    "\n",
    "#Get the test set IoU and print\n",
    "iou_test = IoU(seg_test_onehot,y_pred_test)\n",
    "print(\"Test Set IoU: {}\".format(iou_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Show sample images demonstating segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set plotting variables\n",
    "num_show = 5\n",
    "num_col = 4\n",
    "num_vali = 2200\n",
    "threshold = 0.50\n",
    "\n",
    "#Randomly generate a set of indices to print\n",
    "indices = [i for i in range(num_vali)]\n",
    "sample_indices = sample(indices, num_show)\n",
    "\n",
    "#Print 4 columns of images\n",
    "fig = plt.figure(figsize=(18, num_show*5))\n",
    "for i in range(num_show):\n",
    "    \n",
    "    #Plot US image\n",
    "    a0 = fig.add_subplot(num_show, num_col, i*num_col+1)\n",
    "    img0 = a0.imshow(np.flipud(us_test[sample_indices[i], :, :, 0].astype(np.float32)))\n",
    "    a0.set_title(\"Ultrasound #{}\".format(sample_indices[i]))\n",
    "    \n",
    "    #Plot original segmentations\n",
    "    a1 = fig.add_subplot(num_show, num_col, i*num_col+2)\n",
    "    img1 = a1.imshow(np.flipud(seg_test[sample_indices[i], :, :, 0]), vmin=0.0, vmax=1.0)\n",
    "    a1.set_title(\"Segmentation #{}\".format(sample_indices[i]))\n",
    "    \n",
    "    #Plot Predicted segmentations\n",
    "    c = fig.colorbar(img1, fraction=0.046, pad=0.04)\n",
    "    a2 = fig.add_subplot(num_show, num_col, i*num_col+3)\n",
    "    img2 = a2.imshow(np.flipud(y_pred_test[sample_indices[i], :, :, 1]), vmin=0.0, vmax=1.0)\n",
    "    a2.set_title(\"Prediction #{}\".format(sample_indices[i]))\n",
    "    \n",
    "    #Plot thresholded segmentations\n",
    "    c = fig.colorbar(img2, fraction=0.046, pad=0.04)\n",
    "    a3 = fig.add_subplot(num_show, num_col, i*num_col+4)\n",
    "    img3 = a3.imshow((np.flipud(y_pred_test[sample_indices[i], :, :, 1]) > threshold), vmin=0.0, vmax=1.0)\n",
    "    c = fig.colorbar(img3, fraction=0.046, pad=0.04)\n",
    "    a3.set_title(\"Thresholded #{}\".format(sample_indices[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlenv] *",
   "language": "python",
   "name": "conda-env-dlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
